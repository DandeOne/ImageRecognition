{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simplified code from another .ipynb file in this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_AvgPool(shape):\n",
    "  # This function makes standard vgg model and changing all maxpool layers to average pooling layer\n",
    "  vgg = VGG16(input_shape=shape, weights='imagenet', include_top=False)\n",
    "\n",
    "  new_model = Sequential()\n",
    "  for layer in vgg.layers:\n",
    "    if layer.__class__ == MaxPooling2D:\n",
    "      # replace it with average pooling\n",
    "      new_model.add(AveragePooling2D())\n",
    "    else:\n",
    "      new_model.add(layer)\n",
    "\n",
    "  return new_model\n",
    "\n",
    "def gram_matrix(img):\n",
    "  # input is (H, W, C) (C = # feature maps)\n",
    "  # we first need to convert it to (C, H*W)\n",
    "  X = K.batch_flatten(K.permute_dimensions(img, (2, 0, 1)))\n",
    "  \n",
    "  # now, calculate the gram matrix\n",
    "  # gram = XX^T / N\n",
    "  # the constant is not important since we'll be weighting these\n",
    "  G = K.dot(X, K.transpose(X)) / img.get_shape().num_elements()\n",
    "  return G\n",
    "\n",
    "\n",
    "def style_loss(y, t):\n",
    "  return K.mean(K.square(gram_matrix(y) - gram_matrix(t)))\n",
    "\n",
    "def minimize(fn, epochs, batch_shape):\n",
    "  t0 = datetime.now()\n",
    "  losses = []\n",
    "  x = np.random.randn(np.prod(batch_shape))\n",
    "  for i in range(epochs):\n",
    "    x, l, _ = fmin_l_bfgs_b(\n",
    "      func=fn,\n",
    "      x0=x,\n",
    "      maxfun=20\n",
    "    )\n",
    "    x = np.clip(x, -127, 127)\n",
    "    print(\"iter=%s, loss=%s\" % (i, l))\n",
    "    losses.append(l)\n",
    "\n",
    "  print(\"duration:\", datetime.now() - t0)\n",
    "  plt.plot(losses)\n",
    "  plt.show()\n",
    "\n",
    "  newimg = x.reshape(*batch_shape)\n",
    "  final_img = unpreprocess(newimg)\n",
    "  return final_img[0]\n",
    "\n",
    "def unpreprocess(img):\n",
    "  img[..., 0] += 103.939\n",
    "  img[..., 1] += 116.779\n",
    "  img[..., 2] += 126.68\n",
    "  img = img[..., ::-1]\n",
    "  return img\n",
    "\n",
    "\n",
    "def scale_img(x):\n",
    "  x = x - x.min()\n",
    "  x = x / x.max()\n",
    "  return x\n",
    "\n",
    "# load the content image\n",
    "def load_img_and_preprocess(path, shape=None):\n",
    "  img = image.load_img(path, target_size=shape)\n",
    "\n",
    "  # convert image to array and preprocess for vgg\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "def get_loss_and_grads_wrapper(x_vec):\n",
    "  l, g = get_loss_and_grads([x_vec.reshape(*batch_shape)])\n",
    "  return l.astype(np.float64), g.flatten().astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_img = load_img_and_preprocess(\n",
    "  # '../large_files/caltech101/101_ObjectCategories/elephant/image_0002.jpg',\n",
    "  # 'batman.jpg',\n",
    "  'content/qingdao.jpg'\n",
    "  #'content/sydney.jpg'\n",
    "  # (225, 300),\n",
    "  #  'content/pepe.jpg'\n",
    ")\n",
    "\n",
    "# resize the style image\n",
    "h, w = content_img.shape[1:3]\n",
    "style_img = load_img_and_preprocess(\n",
    "  # 'styles/starrynight.jpg',\n",
    "  # 'styles/flowercarrier.jpg',\n",
    "  # 'styles/monalisa.jpg',\n",
    "  #'styles/lesdemoisellesdavignon.jpg',\n",
    "    'styles/wave.jpg',\n",
    "    (h,w)\n",
    ")\n",
    "\n",
    "\n",
    "# we'll use this throughout the rest of the script\n",
    "batch_shape = content_img.shape\n",
    "shape = content_img.shape[1:]\n",
    "\n",
    "\n",
    "# we want to make only 1 VGG here\n",
    "# as you'll see later, the final model needs\n",
    "# to have a common input\n",
    "vgg = VGG16_AvgPool(shape)\n",
    "\n",
    "\n",
    "# create the content model\n",
    "# we only want 1 output\n",
    "# remember you can call vgg.summary() to see a list of layers\n",
    "# 1,2,4,5,7-9,11-13,15-17\n",
    "content_model = Model(vgg.input, vgg.layers[14].get_output_at(0))\n",
    "content_target = K.variable(content_model.predict(content_img))\n",
    "\n",
    "\n",
    "# create the style model\n",
    "# we want multiple outputs\n",
    "# we will take the same approach as in style_transfer2.py\n",
    "symbolic_conv_outputs = [\n",
    "  layer.get_output_at(1) for layer in vgg.layers \\\n",
    "  if layer.name.endswith('conv1')\n",
    "]\n",
    "\n",
    "# make a big model that outputs multiple layers' outputs\n",
    "style_model = Model(vgg.input, symbolic_conv_outputs)\n",
    "\n",
    "# calculate the targets that are output at each layer\n",
    "style_layers_outputs = [K.variable(y) for y in style_model.predict(style_img)]\n",
    "\n",
    "# we will assume the weight of the content loss is 1\n",
    "# and only weight the style losses\n",
    "style_weights = [0.2,0.4,0.3,0.5,0.2]\n",
    "\n",
    "\n",
    "\n",
    "# create the total loss which is the sum of content + style loss\n",
    "loss = K.mean(K.square(content_model.output - content_target))\n",
    "\n",
    "for w, symbolic, actual in zip(style_weights, symbolic_conv_outputs, style_layers_outputs):\n",
    "  # gram_matrix() expects a (H, W, C) as input\n",
    "  loss += w * style_loss(symbolic[0], actual[0])\n",
    "\n",
    "\n",
    "# once again, create the gradients and loss + grads function\n",
    "# note: it doesn't matter which model's input you use\n",
    "# they are both pointing to the same keras Input layer in memory\n",
    "grads = K.gradients(loss, vgg.input)\n",
    "\n",
    "# just like theano.function\n",
    "get_loss_and_grads = K.function(\n",
    "  inputs=[vgg.input],\n",
    "  outputs=[loss] + grads\n",
    ")\n",
    "\n",
    "\n",
    "final_img = minimize(get_loss_and_grads_wrapper, 10, batch_shape)\n",
    "plt.imshow(scale_img(final_img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave('saved/style_and_content.png',scale_img(final_img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
